<!DOCTYPE HTML>
<!--
	Tabular Data Analysis Workshop @ VLDB 2024
	Template from html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
	<title>Trustworthy Foundation Models 2024</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<link rel="stylesheet" href="assets/css/main.css" />
	<noscript>
		<link rel="stylesheet" href="assets/css/noscript.css" />
	</noscript>
	<!-- Google tag (gtag.js) -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=xxxxx"></script>
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag() { dataLayer.push(arguments); }
		gtag('js', new Date());

		gtag('config', 'G-xxxxx');
	</script>
	
	
 <style>
  /* 仅作用于 square div 下的内容 */
  		.square {
  			margin: 16px;
  			text-align: center;
  		}
  
  		.square h5 {
  			color: green;
  		}
  
  		.square img {
  			float: left;
  			margin: 3px;
  			height: 120px; 
  			width: 120px; 
  			object-fit: fill;
  			border-radius: 10%
  		}
  
  		.square p {
  			text-align: justify;
  			font-size: 16px;
  		}
        </style>

</head>

<body class="is-preload">

	<!-- Wrapper -->
	<div id="wrapper">

		<!-- Header -->
		<header id="header" class="alt">
			<span class="logo"><img src="images/logo.jpeg" alt="" /></span>
			<h1 property="dc:title">Tokyo Workshop on<br />Trustworthy Foundation Models <br>(TFM 2024)</h1>
			<p>
				Tokyo Institute of Technology, Tokyo, Japan 
				<br> August 19-20, 2024
				<br> Organized by <a href="https://cao-lab.org">TDSAI Lab</a>.
			</p>
		</header>

		<!-- Nav -->
		<nav id="nav">
			<ul>
				<li><a href="#objectives" class="active">Introduction</a></li>
				<li><a href="#speakers">Invited Speakers</a></li>
				<li><a href="#program">Program</a></li>
				<li><a href="#participant">Participant</a></li>
				<li><a href="#location">Location</a></li>
			</ul>
		</nav>

		<!-- Main -->
		<div id="main">


			<!-- Objectives -->
			<section id="objectives" class="main">
				<div class="spotlight">
					<div class="content">
						<header class="major">
							<h2>Introduction</h2>
						</header>
						<p>
							Welcome to the workshop on trustworthy foundation models 2024! This is an academic research event dedicated to exploring the transformative impact of these models on society, life, and research. 
							As foundation models continue to integrate into various aspects of our daily lives, numerous trust issues have emerged, including but limited to concerns related to privacy, security, fairness, explainability, hallucination, bias, etc. 
							This workshop aims to foster a collaborative environment by inviting esteemed researchers from both academia and industry to discuss the latest developments and ongoing challenges in creating trustworthy foundation models. 
						</p>
					</div>
				</div>
			</section>

		

			<!-- Submissions -->
			<section id="speakers" class="main">
				<div class="content">
					<header class="major">
						<h2>Invited Speakers</h2>
					</header>
					
					

  <h3>
    <p align="center"> 
    <b>
    Prof. Xiaochun Cao (SYSU) 
    <br>
      Talk Title: Trustworthy Computer Vision Models
    </b> 
    </p>
  </h3>
   

      <div>
    
        <p>
          <b>Abstract</b>: Computer vision tasks range from the simple perspective projection matrix estimation in a traditional camera calibration application to the large-scale foundation model fitting in a contemporary object detection cloud service. One may solve most computer vision tasks through fitting functions mapping the dense, if not continuous due to quantization, visual input to a discrete and meaningful output space, including categories, bounding boxes, and depths. Due to the significant difference in cardinalities of the domain and codomain, these mapping functions fail to meet one of the three Hadamard criteria for being well-posed. In other words, the unstable computer vision solution does not depend continuously on the parameters or input data. Many researchers are trying their best to design or learn computer vision algorithms being sufficiently robust to complex perturbations such as occlusion, smoke, rain, and fog. There are also scholars looking for dedicated but powerful adversarial perturbations. Does there exist an invariant backdoor perturbation that is capable to push an arbitrary image across the decision boundary in a classification task? Are all perturbations adversarial? Why do the AI models succeed? Fail? In this talk, I will introduce these questions our team is exploring and briefly outline some of the progress. However, much remains unclear in spite of our efforts, and we reiterate that there might not have the answers we're looking for before AI undergoes a brand-new paradigm shift.</p>
      </div>
      <div class="square">
       
        <div>
          <img src="pic/xiaochun.jpeg">
        </div>
        <p>
          <b>Biography</b>: Dr. Xiaochun Cao is the Dean of School of Cyber Science and Technology, Sun Yat-sen University. His research interests include artificial intelligence especially computer vision, and content analysis in cyber space, etc. He received the B.E. and M.E. degrees both in computer science from Beihang University (BUAA), China, and the Ph.D. degree in computer science from the University of Central Florida, USA, with his dissertation nominated for the university level Outstanding Dissertation Award. Before joining SYSU, he was a professor at Institute of Information Engineering, Chinese Academy of Sciences. He has authored and coauthored over 300 journal and conference papers. In 2004 and 2010, he was the recipients of the Piero Zamperoni best student paper award at the International Conference on Pattern Recognition.
          <br>Dr. Cao was the recipients of Outstanding Young Scientists Fund and Excellent Young Scientists Fund of National Natural Science Foundation of China, in 2020 and 2014, respectively. He is on the editorial boards of IEEE Transactions on Pattern Analysis and Machine Intelligence, IEEE Transactions on Image Processing, IEEE Transactions on Multimedia, and Acta Electronica Sinica. He was on the editorial board of IEEE Transactions on Circuits and Systems for Video Technology. Four of Prof. Cao's former Ph.D. students, Xiaojie Guo(2013), Changqing Zhang(2016), Xiao wang(2016), and Wenqi Ren(2017), were the recipients of Excellent Young Scientists Fund of National Natural Science Foundation of China or National High-level personnel of special support program. Three of his former Ph.D. students, Wenqi Ren(2017), Ke Ma(2019), Pengwen Dai(2022), were the winners of China Computer Federation (CCF)/Chinese Institute of Electronics(CIE)/Chinese Academy of Sciences(CAS)/ Doctoral Dissertation Awards.</p>
      </div>

	
	
	
			
					
</section>


				<!-- Program -->
			<section id="program" class="main">
			
		
				<div class="content">
					<header class="major">
						<h2>Program</h2>
					</header>
					(TBD)
					<br>
					The workshop will include invited talks and panel discussion.
					
					
					<p>
					</p>
				</div>
			</section>
			
			
			<!-- Participant -->
			<section id="participant" class="main">
			
		
				<div class="content">
					<header class="major">
						<h2>Participant</h2>
					</header>
					This event is free to attend. However, there will be a participant fee for those who wish to attend the welcome party. 
					<br>For more information, please contact <a href="mailto:cao@c.titech.ac.jp?subject=Registration for TFM2024">cao@c.titech.ac.jp</a>.
					
					<p>
					</p>
				</div>
			</section>

		


			<!-- Location -->
			<section id="location" class="main">
				<div class="content>
					<header class="major">
						<h2>Location</h2>
					</header>
					Tokyo Institute of Technology,Okayama Campus, West Bldg. 9, Collaboration Room.
					<br>
					(東京工業大学 西9号館 コラボレーションルーム)
					<br>
					See <a href="https://maps.app.goo.gl/WNgjb2Rj7BbGdg4c9">Google Map</a>
					<br>
					
					<iframe src="https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d810.9721628499583!2d139.68179028269205!3d35.60581316021722!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x6018f5c7cc7bd92d%3A0xc8499216b1e3dd28!2z5p2x5Lqs5bel5qWt5aSn5a2mIOilvznlj7fppKg!5e0!3m2!1sen!2sat!4v1721857533298!5m2!1sen!2sat" width="300" height="200" style="border:0;" allowfullscreen="" loading="lazy" referrerpolicy="no-referrer-when-downgrade"></iframe>
				</div>	
			</section>

		</div>

		<!-- Footer -->
		<footer id="footer">
			<section>
				
				
			</section>
		</footer>

	</div>

	<!-- Scripts -->
	<script src="assets/js/jquery.min.js"></script>
	<script src="assets/js/jquery.scrollex.min.js"></script>
	<script src="assets/js/jquery.scrolly.min.js"></script>
	<script src="assets/js/browser.min.js"></script>
	<script src="assets/js/breakpoints.min.js"></script>
	<script src="assets/js/util.js"></script>
	<script src="assets/js/main.js"></script>

</body>

</html>
